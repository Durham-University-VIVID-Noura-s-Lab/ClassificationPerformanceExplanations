{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to set up and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import argparse\n",
    "import pickle as pk\n",
    "import time\n",
    "from re import S\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data_utils import *\n",
    "#from src.model_utils import setupTokenizer\n",
    "from src.datasethandler import NarrationDataSet,train_data_permutated_path,test_data_path,DataSetLoader,ClassificationReportPreprocessor\n",
    "from src.inferenceUtils import PerformanceNarrator\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback,T5Config\n",
    "from src.modeling_bart import BartNarrationModel\n",
    "from src.modeling_t5 import T5NarrationModel\n",
    "from src.trainer_utils import getTrainingArguments,CustomTrainerFusion,get_model\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters used to set up the models\n",
    "modeltype = 'earlyfusion'  # either baseline or 'earlyfusion'\n",
    "\n",
    "# either t5-small,t5-base, t5-large, facebook/bart-base, or facebook/bart-large\n",
    "modelbase = 'facebook/bart-base'\n",
    "\n",
    "# we will use the above variables to set up the folder to save our model\n",
    "pre_trained_model_name = modelbase.split(\n",
    "    '/')[1] if 'bart' in modelbase else modelbase\n",
    "\n",
    "# where the trained model will be saved\n",
    "output_path = 'TrainModels/' + modeltype + '/'+pre_trained_model_name+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,529 training samples\n",
      "  100 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Using the dataset used in the paper\n",
    "dataset_raw= DataSetLoader() \n",
    "\n",
    "# Process the data and set up the tokenizer\n",
    "narrationdataset = NarrationDataSet(modelbase,\n",
    "                                    max_preamble_len=160,\n",
    "                                    max_len_trg=185, max_rate_toks=8,\n",
    "                                    lower_narrations=True,\n",
    "                                    process_target=True)\n",
    "\n",
    "narrationdataset.fit(dataset_raw.train_data, dataset_raw.test_data)\n",
    "\n",
    "dataset = narrationdataset.train_dataset\n",
    "test_dataset = narrationdataset.test_dataset\n",
    "tokenizer = tokenizer_ = narrationdataset.tokenizer_\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = dataset, test_dataset\n",
    "\n",
    "\n",
    "train_size = int(len(dataset))\n",
    "val_size = int(len(test_dataset))\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments/parameters to train the model\n",
    "\n",
    "arguments = train_arguments = {'output_dir': output_path,\n",
    "                               'warmup_ratio': 0.2,\n",
    "                               'per_device_train_batch_size': 8,\n",
    "                               'num_train_epochs': 10,\n",
    "                               'lr_scheduler_type': 'cosine',\n",
    "                               'learning_rate': 5e-5,\n",
    "                               'evaluation_strategy': 'steps',\n",
    "                               'logging_steps': 500,\n",
    "                               'seed': 456}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(456)\n",
    "device = torch.device( 'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Build actual trainingArgument object\n",
    "training_arguments = getTrainingArguments(train_arguments)\n",
    "\n",
    "getModel = get_model(narrationdataset,model_type=modeltype)\n",
    "\n",
    "\n",
    "trainer = CustomTrainerFusion(model_init=getModel,\n",
    "                        args=training_arguments,\n",
    "                        train_dataset=narrationdataset.train_dataset,\n",
    "                        eval_dataset=narrationdataset.test_dataset,\n",
    "                        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)])\n",
    "\n",
    "# Train the narrator\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the model with the lowest evaluation loss\n",
    "trainer.save_model()\n",
    "trainer.save_state()\n",
    "\n",
    "# get the best checkpoint\n",
    "best_check_point = trainer.state.best_model_checkpoint\n",
    "\n",
    "\n",
    "params_dict = train_arguments\n",
    "\n",
    "params_dict['best_check_point'] = best_check_point\n",
    "params_dict['output_path'] = output_path\n",
    "json.dump(params_dict, open(f'{output_path}/parameters.json', 'w'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TrainModels/earlyfusion/bart-base/checkpoint-1500'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2032346725463867,\n",
       " 'eval_runtime': 0.572,\n",
       " 'eval_samples_per_second': 174.815,\n",
       " 'eval_steps_per_second': 22.726,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resut = trainer.evaluate()\n",
    "resut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Initialising the narrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrator = PerformanceNarrator(trainer.model,narrationdataset,device,sampling=False,verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset_raw.test_data[99]\n",
    "seed = 456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 456\n"
     ]
    }
   ],
   "source": [
    "outt=narrator.generateNarration(example, seed,  max_length=190,\n",
    "                          length_penalty=8.6, beam_size=10,\n",
    "                          repetition_penalty= 3.5,\n",
    "                           return_top_beams=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"On this multi-class classification problem, the model was trained to assign test samples one of the following classes #CA, #CB, #CC, and #CC. The classifier's performance assessment scores are as follows: (a) Accuracy equal to 76.44%. (b) F1score equal to about76.03%. These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true label for several test examples with only a small margin of error. Furthermore, from the precision and recall scores, we can draw the conclusion that it will likely have a lower misclassification error rate.\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('annotation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
