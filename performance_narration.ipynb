{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import argparse\n",
    "import pickle as pk\n",
    "import time\n",
    "from re import S\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.data_utils import *\n",
    "#from src.model_utils import setupTokenizer\n",
    "from src.datasethandler import NarrationDataSet,train_data_permutated_path,test_data_path,DataSetLoader\n",
    "from src.inferenceUtils import PerformanceNarrator\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback,T5Config\n",
    "from src.modeling_bart import BartNarrationModel\n",
    "from src.modeling_t5 import T5NarrationModel\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_performance_description_inference import ClassificationPerformanceNarration\n",
    "trained_model_path = 'TrainModels/earlyfusion/bart-base/'\n",
    "#\"Trained_models/trainednarrators/baseline/bart-base/\"\n",
    "\n",
    "modeltype='earlyfusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type: earlyfusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DataNarrationBart were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.relative_attention_for_table.key.weight', 'encoder.relative_attention_for_table.value.weight', 'encoder.relative_attention_for_table.key.bias', 'encoder.relative_attention_for_table.value.bias', 'encoder.relative_attention_for_table.query.bias', 'encoder.relative_attention_for_table.query.weight', 'encoder.relative_attention_for_table.mask', 'encoder.relative_attention_for_table.Er']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# instantiate the narrator\n",
    "narrator = ClassificationPerformanceNarration(model_base = \"facebook/bart-base\",\n",
    "                                              model_base_dir= trained_model_path,\n",
    "                                              \n",
    "                                              modeltype=modeltype,\n",
    "                                              max_preamble_len=160,\n",
    "                                              max_len_trg=185,\n",
    "                                              length_penalty=3.6,\n",
    "                                              beam_size=8,\n",
    "                                              repetition_penalty=1.5,\n",
    "                                              return_top_beams=1,\n",
    "                                              random_state=453,\n",
    "                                              )\n",
    "                                              \n",
    "# Set up the inference model\n",
    "narrator.buildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The scores achieved by the model on this classification task are as follows: (a) AUC: 50.16%. (b) Recall: 19.56%. These scores are lower than expected, and judging by them, we can conclude that this model has a lower performance as it is not be able to accurately predict the actual labels of multiple test examples. Furthermore, the Accuracy score is only marginally higher than the dummy model constantly assigning the majority class label \"Not Fraud\" to any given test case.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The setup is ready to be used\n",
    "\n",
    "# Sample of the expected format for the performance report\n",
    "performance_report = {'Recall': [\"19.56%\", \"LOW\"],\n",
    "                      'F1-score': [\"9.06%\",\n",
    "                                   \"LOW\"],\n",
    "                      'Accuracy': [\"68.16%\", \"LOW\"],\n",
    "                      'AUC': [\"50.16%\", \"MODERATE\"]\n",
    "                      }\n",
    "class_labels = ['Not Fraud', 'Fraud']\n",
    "is_balance = False\n",
    "narrator.beam_size = 4\n",
    "generatedTexts = narrator.generateTextualExplanation(performance_report, class_labels,\n",
    "                                                     is_balance=is_balance)\n",
    "generatedTexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The model's classification performance on this binary classification task as evaluated based on the Precision, Sensitivity, AUC, and Accuracy are 85.56%, 79.23%, 89.78%, and 87.24%, respectively. These scores support the conclusion that this model will be moderately effective at correctly labeling most test samples drawn from any of the two-class labels. Furthermore, the likelihood of misclassifying any given test example is lower.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2\n",
    "\n",
    "# Sample of the expected format for the performance report\n",
    "performance_report = {'Precision': [\"85.56%\", \"HIGH\"],\n",
    "                      'Sensitivity': [\"79.23%\", \"HIGH\"],\n",
    "                      'Accuracy': [\"87.24%\", \"HIGH\"],\n",
    "                      'AUC': [\"89.78%\", \"HIGH\"]\n",
    "                      }\n",
    "class_labels = ['Reject', 'Accept']\n",
    "is_balance = True\n",
    "generatedTexts = narrator.generateTextualExplanation(performance_report, class_labels,\n",
    "                                                     is_balance=is_balance)\n",
    "generatedTexts\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('annotation')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
